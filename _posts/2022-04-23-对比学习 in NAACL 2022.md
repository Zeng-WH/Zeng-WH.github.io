---
layout:     post
title:      "对比学习 in NAACL 2022"
subtitle:   ACL冲冲冲！
date:       2022-04-23 19:00:00
author:     "Andrew Zeng"
tags:
      - 对比学习


---

将陆续更新NAACL 2022中与对比学习相关的论文。

# DiffCSE：Difference-based Contrastive Learning for SentenceEmbeddings

## 1. Motivation
通过对比学习来学习sentence embeddings的表示。一般的思路：对于单个样本利用multiple augmentations的方法来构造positive pairs. 这些方法的训练目标在于让representations对于augmentation transformation是invariant的。（比如常用的dropout-based的方法）
然而有些的augmentation的方法（如对input进行deletion或者replacement）往往会改变原句的意思，因此，理想的augmentation方法应当对于这些transformations应当是not invariat的。
在计算机视觉领域，此为equivariantcontrastive learning，通过在不敏感的image transformations（如灰度变化）使用对比损失，而在敏感的image transformations(如图片的旋转)上使用prediction loss.

##  2. Contribution
实验表明DiffCSE在unsupervised sentence representa-tion learning methods达到了SOTA的效果。在se-mantic textual similarity tasks上超越SimCSE 2.3个absolute points.

## 3. Difference-based Constrastive Learning
作者的方法很简单，将SimCSE中标准的对比学习目标域与基于sentence embeddings的差异预测目标结合。

![75b0ad2331b342aeadfda2701d99a134](https://user-images.githubusercontent.com/47687248/164890143-478b82bb-7cd5-4532-9cdc-b6f50fdbb39d.png)

上图的左边即为SimCSE的训练目标：

![4abc5d9cc8014871aea3f74f96b0cdc4](https://user-images.githubusercontent.com/47687248/164890151-a2f42331-3afa-43c7-9b91-f3c6ac70bddc.png)

上图的右边ELECTRA中差异预测训练目标的conditional版本。包括generator和discriminator.
对于给定的长度为T的句子x, 在x上进行随机的mask以获得$x^{'}$, 使用预训练好的MLM模型作为generator来恢复mask tokens，得到$x^{''}$，使用discriminator来进行替换的token检测的任务（RTD），对于句子中的每一个token，模型需要预测该token是否被替换。

![dab04a0c6160462abd04d5f5ab10c00c](https://user-images.githubusercontent.com/47687248/164890158-07fc8a38-2e8d-4325-9330-47d01ea9caf7.png)

最终的训练目标：

![9778a327a4fd47d09bd26b5aab58a052](https://user-images.githubusercontent.com/47687248/164890165-894c2801-9096-43b4-a822-a85cba08219b.png)

Discriminator的梯度会反向传播到sentence encoder上，使得sentence encoder能够包含句子x的完整意思，从而使Discriminator能够区分x和$x^{''}$的细微区别。

训练过程中固定generator的参数，而优化sentence encoder和discriminator. 在获取sentence embedding时，只需要sentence encoder的输出。

# Learning Dialogue Representations from Consecutive Utterances

看到了简单且有效的Dialogue Representation的方法。

## 1. 介绍

学习高质量的对话表示对于解决一系列的面向对话的任务是非常重要的，尤其是考虑到对话系统会遇到数据稀疏的问题。之前的监督式对比学习方法在学习对话表示时，可以使用Dialogue-NLI数据集来构造对比学习中的样本对。但由于其规模和多样性的限制，得到的模型表现并不令人满意。而无监督对比学习的方法，如SimCSE，TOD-BERT分别是一般文本和对话中的SOTA. 将SimCSE直接用在对话数据上表现并不好，作者分析可能需要构造更好的样本对。而TOD-BERT在构造postive pair时，会将所有之前的utterance拼接起来构造positive pair, 局限性太大。

## 2. 模型（DSE）

作者的构造样本对的方法很简单，将连续的utterance拼接起来构造postive pair.

作者采用Hard-Negative sampling策略，损失函数如下所示：

如上所示，$i$与$i^{+}$代表anchor和正样本，给难分的负样本增加权重

由于$i$与$i^{+}$的位置可以互换，损失函数可以计算为：

## 3. 实验

虽然作者的方法很简单，但实验表明作者的方法很有效。作者比较当前的SOTA模型与DSE在Intent Classification，Out-of-scope Detection，Utterance-level Response Selection，Dialogue-Level Response Selection与Dialogue Action Prediction的表现。

可以看到无论是Similar-based还是finetune的方法，DSE都表现优异。



